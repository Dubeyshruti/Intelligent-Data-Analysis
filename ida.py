# -*- coding: utf-8 -*-
"""ida.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DvXdW5-mfrSaRT_EsCqKehzxdQ6CFXia
"""

#null values
#outlier analysis using IQR technique

1. Demonstration of different pre-processing techniques including missing value
   handling and data discretization on Adult dataset.

2. Demonstration of data reduction techniques including PCA and Histogram on Predict
   students' dropout and academic success dataset.

3. Demonstration of classification rules process on dataset of your choice using ID3
   and J48 algorithm in Weka explorer.

4. Implement the classification rules process on dataset of your choice using Naive
   Baye's algorithm in Weka explorer.

5. Build a Neural Network model to predict whether tumor is malignant or benign for
   Breast Cancer Wisconsin (Diagnostic) dataset using Python.

6. Demonstration of clustering on dataset of your choice using simple K-means
   algorithm in weka explorer and python.

7. Demonstration of clustering on dataset of your choice using simple DBSCAN
   algorithm in weka explorer and python.

8. Demonstration of clustering on dataset of your choice using simple BIRCH algorithm
   in weka explorer and python.

9. Demonstration of association rule generation on Groceries dataset for Market Basket
   Analysis using Apriori algorithm in weka explorer.

10. Perform comparative analysis of Apriori and FP-Growth algorithms on Market Basket
    Analysis using Python

"""LAB 1

Demonstration of different pre-processing techniques including missing value handling and data discretization on Adult dataset.
"""

import pandas as pd

df=pd.read_csv("/content/drive/MyDrive/netflix1.csv")

df

df.isnull()

df.isnull().sum()

df.boxplot()

data=pd.read_csv("/content/drive/MyDrive/UPELECTIONS.csv")
data

data.isnull()

data.isna()

newdata=data.dropna()

data.isnull().sum()

data.isna().sum()

data.isna().describe()

data.drop_duplicates()

new_hashtag={["up elections"]}
nd=data.fillna(new_hashtag)

data.sort_values(["date"])

data["hashtags"]

data.filter(['hashtags'])

data.filter(['user_name'])

data.count()

data.filter(['user_location'])

data.boxplot()

s=data['source']
s

data.mean()

"""LAB 2

Demonstration of data reduction techniques including PCA and Histogram on Predict students' dropout and academic success dataset.
"""

data=[[12,45,67,87,55,76],
      [23,56,43,22,45,22],
      [45,67,54,33,22,54]
      ]

df=pd.DataFrame(data,columns=['IDA','CC','PR-ML','NLP','CNS','PD'],index=['student1','student2','student3'])
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

df.shape

import pandas as pd
df

scaler = StandardScaler()
standardized_data = scaler.fit_transform(df)

pca = PCA(n_components=3)
pca_result = pca.fit_transform(standardized_data)
print(pca_result)

# Commented out IPython magic to ensure Python compatibility.
pd.plotting.register_matplotlib_converters()
import matplotlib.pyplot as plt
# %matplotlib inline

plt.figure(figsize=(8, 6))
plt.hist(df, bins=20, alpha=0.7)
plt.xlabel('Marks')
plt.ylabel('pca_result')
plt.title('Histogram Plot')
plt.grid(True)
plt.show()

# Create a scatterplot of the reduced data
plt.scatter(pca_result[:, 0], pca_result[:, 1])
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('Reduced Data')
plt.show()

"""LAB3

Demonstration of classification rules process on dataset of your choice using ID3 and J48 algorithm in Weka explorer.
"""

# Import necessary libraries
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz
from sklearn.model_selection import train_test_split
import graphviz

# Load the Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# ID3 Algorithm
id3_classifier = DecisionTreeClassifier(criterion='entropy', random_state=42)
id3_classifier.fit(X_train, y_train)

# J48 (C4.5) Algorithm
j48_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)
j48_classifier.fit(X_train, y_train)

# Print the ID3 decision tree rules
id3_rules = export_text(id3_classifier, feature_names=iris.feature_names)
print("ID3 Decision Tree Rules:\n", id3_rules)

# Print the J48 (C4.5) decision tree rules
j48_rules = export_text(j48_classifier, feature_names=iris.feature_names)
print("J48 (C4.5) Decision Tree Rules:\n", j48_rules)

# Visualize the ID3 decision tree (requires graphviz)
dot_data_id3 = export_graphviz(id3_classifier, out_file=None, feature_names=iris.feature_names, filled=True, rounded=True)
graph_id3 = graphviz.Source(dot_data_id3)
graph_id3.render("id3_decision_tree")

# Visualize the J48 (C4.5) decision tree (requires graphviz)
dot_data_j48 = export_graphviz(j48_classifier, out_file=None, feature_names=iris.feature_names, filled=True, rounded=True)
graph_j48 = graphviz.Source(dot_data_j48)
graph_j48.render("j48_decision_tree")

from google.colab import drive
drive.mount('/content/drive')

"""LAB4

Implement the classification rules process on dataset of your choice using Naive Baye's algorithm in Weka explorer.
"""

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix

# Load the Iris dataset
from sklearn.datasets import load_iris
iris = load_iris()
X = iris.data
y = iris.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create a Naive Bayes classifier (Gaussian Naive Bayes)
classifier = GaussianNB()

# Train the classifier on the training data
classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = classifier.predict(X_test)

# Evaluate the classifier's performance
confusion_mat = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred, target_names=iris.target_names)

# Print the results
print("Confusion Matrix:\n", confusion_mat)
print("\nClassification Report:\n", classification_rep)

import pandas as pd
import requests
from io import StringIO

url = "https://huggingface.co/datasets/scikit-learn/breast-cancer-wisconsin/blame/main/breast_cancer.csv"
response = requests.get(url)

if response.status_code == 200:
    data = response.text
else:
    print("Failed to retrieve data. Status code:", response.status_code)

if response.status_code == 200:
    data = response.text
    df = pd.read_csv(StringIO(data))

df = pd.read_csv(StringIO(data), delimiter='\t')

"""LAB5

 Build a Neural Network model to predict whether tumor is malignant or benign for Breast Cancer Wisconsin (Diagnostic) dataset using Python.
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Load the Breast Cancer Diagnosis dataset from Scikit-Learn
from sklearn.datasets import load_breast_cancer
data = load_breast_cancer()

# Convert the dataset to a Pandas DataFrame
df = pd.DataFrame(data.data, columns=data.feature_names)

# Add the target (diagnosis) column to the DataFrame
df['diagnosis'] = data.target

# Split the dataset into features (X) and target (y)
X = df.drop('diagnosis', axis=1)
y = df['diagnosis']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features (important for neural networks)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Build a neural network model using TensorFlow and Keras
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model on the test data
y_pred = (model.predict(X_test) > 0.5).astype(int)

# Calculate accuracy and display the results
accuracy = accuracy_score(y_test, y_pred)
confusion = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Confusion Matrix:\n{confusion}")
print(f"Classification Report:\n{classification_rep}")

# Import necessary libraries
import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Import the dataset from an online resource
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data"
column_names = ["ID", "Diagnosis", "Mean Radius", "Mean Texture", "Mean Perimeter", "Mean Area", "Mean Smoothness",
                "Mean Compactness", "Mean Concavity", "Mean Concave Points", "Mean Symmetry", "Mean Fractal Dimension",
                "SE Radius", "SE Texture", "SE Perimeter", "SE Area", "SE Smoothness", "SE Compactness", "SE Concavity",
                "SE Concave Points", "SE Symmetry", "SE Fractal Dimension", "Worst Radius", "Worst Texture",
                "Worst Perimeter", "Worst Area", "Worst Smoothness", "Worst Compactness", "Worst Concavity",
                "Worst Concave Points", "Worst Symmetry", "Worst Fractal Dimension"]
data = pd.read_csv(url, header=None, names=column_names)

# Drop the ID column as it's not useful for classification
data = data.drop(columns=["ID"])

# Convert the 'Diagnosis' column to binary labels (Malignant = 1, Benign = 0)
data['Diagnosis'] = data['Diagnosis'].map({'M': 1, 'B': 0})

# Split the data into features (X) and labels (y)
X = data.drop(columns=["Diagnosis"])
y = data["Diagnosis"]

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Build a neural network model
model = tf.keras.Sequential([
    tf.keras.layers.Input(shape=(30,)),  # Input layer with 30 features
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')  # Output layer with sigmoid activation for binary classification
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)

# Evaluate the model on the test set
y_pred = (model.predict(X_test) > 0.5).astype("int32")
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Confusion Matrix:\n{conf_matrix}")
print(f"Classification Report:\n{class_report}")

"""LAB 6

Demonstration of clustering on dataset of your choice using simple K-means algorithm in weka explorer and python.
"""

# Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import load_iris

# Load the Iris dataset
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)

# Select two features for simplicity and visualization
X = X[['sepal length (cm)', 'sepal width (cm)']]

# Instantiate the K-means model with 3 clusters
kmeans = KMeans(n_clusters=3)

# Fit the model to the data
kmeans.fit(X)

# Get cluster centers and labels
centers = kmeans.cluster_centers_
labels = kmeans.labels_

# Add cluster labels to the original data
X['Cluster'] = labels

# Visualize the clusters
plt.scatter(X['sepal length (cm)'], X['sepal width (cm)'], c=labels, cmap='viridis')
plt.scatter(centers[:, 0], centers[:, 1], c='red', marker='X', s=200, label='Cluster Centers')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.legend()
plt.title('K-means Clustering of Iris Dataset')
plt.show()

"""LAB 7

Demonstration of clustering on dataset of your choice using simple DBSCAN algorithm in weka explorer and python.
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import DBSCAN
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA

# Load the Iris dataset
iris = load_iris()
X = iris.data

# Apply PCA to reduce the dataset to 2 dimensions for visualization
pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

# Apply DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(X_reduced)

# Visualize the clustering result
unique_labels = np.unique(dbscan_labels)
n_clusters = len(unique_labels) - (1 if -1 in dbscan_labels else 0)

plt.figure(figsize=(8, 6))

# Create an array of colors for plotting data points
colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, n_clusters + 1)]

for i, label in enumerate(unique_labels):
    if label == -1:
        # Outliers are labeled as -1, plot them in black
        plt.scatter(X_reduced[dbscan_labels == label][:, 0], X_reduced[dbscan_labels == label][:, 1], s=20, c='k', label='Outliers')
    else:
        plt.scatter(X_reduced[dbscan_labels == label][:, 0], X_reduced[dbscan_labels == label][:, 1], s=20, c=colors[i], label=f'Cluster {i + 1}')

plt.title(f'DBSCAN Clustering (Estimated {n_clusters} clusters)')
plt.legend(loc='upper right')
plt.show()

"""LAB 8

Demonstration of clustering on dataset of your choice using simple BIRCH algorithm in weka explorer and python.
"""

import numpy as np
from sklearn.cluster import Birch
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# Load the Iris dataset
iris = load_iris()
X = iris.data

# Instantiate the BIRCH clustering model
birch = Birch(threshold=0.5, branching_factor=50)

# Fit the model to the data
birch.fit(X)

# Get the cluster labels for each data point
labels = birch.labels_

# Plot the clustering result (only for the first two features for visualization)
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
plt.xlabel('Sepal Length')
plt.ylabel('Sepal Width')
plt.title('BIRCH Clustering of Iris Dataset')
plt.show()

"""LAB 9

Demonstration of association rule generation on Groceries dataset for Market Basket Analysis using Apriori algorithm in weka explorer.
"""

pip install pandas apriori

!pip install apyori

import pandas as pd
from apyori import apriori

# Read the transaction data from a CSV file
data = pd.read_csv("/content/drive/MyDrive/Groceries_dataset.csv")

# Convert the DataFrame to a list of transactions
transactions = []
for i in range(len(data)):
    transaction = []
    for item in data.columns:
        if data.loc[i, item] == 1:
            transaction.append(item)
    transactions.append(transaction)

# Generate association rules with minimum support of 10% and minimum confidence of 80%
rules = apriori(transactions, min_support=0.1, min_confidence=0.8)

# Print the association rules
for rule in rules:
    antecedent = rule[0]
    consequent = rule[1]
    support = rule[2]
    confidence = rule[3]
    lift = rule[4]

    print(f"Rule: {antecedent} ==> {consequent}")
    print(f"Support: {support}")
    print(f"Confidence: {confidence}")
    print(f"Lift: {lift}")
    print("------------------------")

import pandas as pd
from apyori import apriori

df = pd.read_csv("/content/drive/MyDrive/Groceries_dataset.csv", header=None)



# Convert the dataset into a list of transactions
transactions = []
for i in range(len(df)):
    transactions.append([str(df.values[i, j]) for j in range(len(df.columns)) if pd.notna(df.values[i, j])])

# Apply the Apriori algorithm
min_support = 0.002  # Adjust the minimum support based on your dataset
min_confidence = 0.2  # Adjust the minimum confidence based on your dataset

rules = apriori(transactions, min_support=min_support, min_confidence=min_confidence)

# Display the generated association rules
print("\nAssociation Rules:")
for rule in rules:
    print(rule)

# Define the transactions from the Groceries dataset
groceries_data = [
    ['bread', 'milk', 'beer'],
    ['bread', 'butter'],
    ['milk', 'diapers', 'beer', 'eggs'],
    ['milk', 'bread', 'diapers', 'beer'],
    ['bread', 'diapers', 'beer', 'eggs']
]

# Import the combinations function
from itertools import combinations

# Function to generate candidate itemsets of size k from frequent itemsets of size k-1
def generate_candidates(prev_frequent_itemsets, k):
    candidates = []
    n = len(prev_frequent_itemsets)

    for i in range(n):
        for j in range(i + 1, n):
            itemset1 = set(prev_frequent_itemsets[i])
            itemset2 = set(prev_frequent_itemsets[j])

            # Generate a candidate itemset of size k by combining two itemsets of size k-1
            new_candidate = list(itemset1.union(itemset2))

            # Only consider unique itemsets
            if len(new_candidate) == k and new_candidate not in candidates:
                candidates.append(new_candidate)

    return candidates

# Function to prune candidate itemsets based on the subset property
def prune_candidates(candidates, prev_frequent_itemsets):
    pruned_candidates = []

    for candidate in candidates:
        subsets = [set(item) for item in combinations(candidate, len(candidate) - 1)]

        # Check if all subsets are in the previous frequent itemsets
        if all(subset in prev_frequent_itemsets for subset in subsets):
            pruned_candidates.append(candidate)

    return pruned_candidates

# Function to find frequent itemsets using the Apriori algorithm
def apriori(transactions, min_support):
    # Count the occurrences of each item in the transactions
    item_counts = {}
    for transaction in transactions:
        for item in transaction:
            item_counts[item] = item_counts.get(item, 0) + 1

    # Identify frequent 1-itemsets
    frequent_itemsets = [[item] for item, count in item_counts.items() if count >= min_support]
    k = 2

    while frequent_itemsets:
        # Generate candidate itemsets of size k
        candidates = generate_candidates(frequent_itemsets, k)

        # Prune candidate itemsets
        candidates = prune_candidates(candidates, frequent_itemsets)

        # Count the occurrences of each candidate itemset in the transactions
        candidate_counts = {tuple(candidate): 0 for candidate in candidates}
        for transaction in transactions:
            for candidate in candidates:
                if set(candidate).issubset(set(transaction)):
                    candidate_counts[tuple(candidate)] += 1

        # Identify frequent k-itemsets
        frequent_itemsets = [list(candidate) for candidate, count in candidate_counts.items() if count >= min_support]

        k += 1

    return frequent_itemsets

# Function to generate association rules from frequent itemsets
def generate_association_rules(frequent_itemsets, min_confidence):
    rules = []

    for itemset in frequent_itemsets:
        if len(itemset) > 1:
            for i in range(1, len(itemset)):
                antecedent = itemset[:i]
                consequent = itemset[i:]
                confidence = item_counts[tuple(itemset)] / item_counts[tuple(antecedent)]

                if confidence >= min_confidence:
                    rules.append((antecedent, consequent, confidence))

    return rules

# Set the minimum support and confidence thresholds
min_support = 2
min_confidence = 0.5

# Find frequent itemsets using Apriori
frequent_itemsets = apriori(groceries_data, min_support)

# Generate association rules
item_counts = {}
for transaction in groceries_data:
    for item in transaction:
        item_counts[item] = item_counts.get(item, 0) + 1

association_rules = generate_association_rules(frequent_itemsets, min_confidence)

# Display the results
print("Frequent Itemsets:")
for itemset in frequent_itemsets:
    print(itemset)

print("\nAssociation Rules:")
for rule in association_rules:
    antecedent = ', '.join(rule[0])
    consequent = ', '.join(rule[1])
    confidence = rule[2]
    print(f"{antecedent} -> {consequent} (Confidence: {confidence:.2f})")

"""LAB 10

Perform comparative analysis of Apriori and FP-Growth algorithms on Market Basket Analysis using Python.
"""

# Import necessary libraries
import pandas as pd
from mlxtend.frequent_patterns import apriori, fpgrowth
from mlxtend.preprocessing import TransactionEncoder
import time

# Generate some example data (replace this with your actual dataset)
data = pd.read_csv("/content/drive/MyDrive/Groceries_dataset.csv")

# Convert the data to the required format
te = TransactionEncoder()
te_ary = te.fit(data).transform(data)
df = pd.DataFrame(te_ary, columns=te.columns_)

# Measure time taken by Apriori
start_time = time.time()
apriori_result = apriori(df, min_support=0.2, use_colnames=True)
apriori_time = time.time() - start_time

# Measure time taken by FP-Growth
start_time = time.time()
fp_growth_result = fpgrowth(df, min_support=0.2, use_colnames=True)
fp_growth_time = time.time() - start_time

# Display the results
print("Apriori Result:")
print(apriori_result)
print("\nTime taken by Apriori: {:.4f} seconds".format(apriori_time))

print("\nFP-Growth Result:")
print(fp_growth_result)
print("\nTime taken by FP-Growth: {:.4f} seconds".format(fp_growth_time))

# Import necessary libraries
import pandas as pd
from mlxtend.frequent_patterns import apriori, fpgrowth
from mlxtend.preprocessing import TransactionEncoder
import time

# Generate some example data (replace this with your actual dataset)
data = [['Milk', 'Bread', 'Butter'],
        ['Bread', 'Butter'],
        ['Milk', 'Diapers', 'Beer', 'Eggs'],
        ['Milk', 'Bread', 'Diapers', 'Beer'],
        ['Bread', 'Diapers', 'Beer', 'Eggs']]

# Convert the data to the required format
te = TransactionEncoder()
te_ary = te.fit(data).transform(data)
df = pd.DataFrame(te_ary, columns=te.columns_)

# Measure time taken by Apriori
start_time = time.time()
apriori_result = apriori(df, min_support=0.2, use_colnames=True)
apriori_time = time.time() - start_time

# Measure time taken by FP-Growth
start_time = time.time()
fp_growth_result = fpgrowth(df, min_support=0.2, use_colnames=True)
fp_growth_time = time.time() - start_time

# Display the results
print("Apriori Result:")
print(apriori_result)
print("\nTime taken by Apriori: {:.4f} seconds".format(apriori_time))

print("\nFP-Growth Result:")
print(fp_growth_result)
print("\nTime taken by FP-Growth: {:.4f} seconds".format(fp_growth_time))

